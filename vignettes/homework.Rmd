---
title: "Homework of all time"
author: "22081"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Homework of all time}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Homework of the semester

There are 11 times in total

## 1st

## Question

*Use knitr to produce at least three examples* (texts,figures,tables).

## Answer

**1.text part**

```{r}
s = "This is THE Euler's Formular"
print(s)

```
${\rm e}^{i\theta} = cos\theta + isin\theta$

**2.figure part**

**objective** *construct a color disk with a pie chart*

First we adjust some marginal distances.

Then we use function "pie" to finish the job with 1000 colors chosen by function "rainbow".
```{r , warning=FALSE, message=FALSE}
par(mar = rep(0,4))
cols <- rainbow(1000) #pick colors from rainbow the function
pie(rep(1,1000),labels = NA,col = cols,border = cols)
```


## 2nd

## Question

Exercise **3.3**, **3.7**, **3.12** and **3.13**(pages 94-96,Statistical Computating with R)


## Answer
**(1) 3.3**

**First**  Use the inverse transform method to simulate a random sample from the Pareto(2, 2) distribution  
**Then**  Superimpose the Pareto(2, 2) density function itself on the histogram generated by those random numbers.
```{r}
n <- 1000
set.seed(777)  #set a seed
u <- runif(n)
x <- 2/sqrt(1-u) #generate the random numbers
hist(x, 
     freq = F, 
     main =expression(f(x) == 8 / x ^ 3), 
     xlim = c(0 , 40), 
     col = "Light Blue", 
     border = "White",
     xlab = "random numbers",
     las = 1)
check<- seq(0 , 40 , .01)
lines( check , 
       8 / (check ^ 3),
       lwd = 3)
```

**Conclusion** From the figure we can see that the empirical and theoretical distributions approximately agree.

**(2) 3.7**

**First**  Write a function as the question instructed to generate a random sample

**Then**  Generate a sample using the parameters given below 

**And last but not least**  Graph the empirical histogram with a theoretical density superimposed.
```{r}
set.seed(329)  #set a seed
betafun <- function(n, alpha, beta)
{
a <- alpha
b <- beta
lim <-  n     #acquire all the parameters from the function
k <- 0
y <- numeric(lim)
while(k < lim){
    x <- runif(1)
    if( x^(a - 1) * (1 - x)^(b - 1)> runif(1))
    { 
        k <- k + 1
        y[k] <- x
      }
} #generate random numbers via acceptance-rejection method
   return(y)
}#construct a function which makes a given numbers of random sample from Beta(a, b)
brdm = betafun(1000, 3, 2) #use the function to produce 1000 random numbers
hist(brdm,
     freq = F,
     main = expression(f(x) == 12 * x^2 *(1 - x)),
     ylim = c(0 , 2),
     col = "Light Blue",
     border = "White",
     las = 1,
     xlab = "Beta random numbers"
     )
check <- seq(0, 1, .01 )
lines(check, 
      12 * check ^ 2 * (1 - check), 
      lwd = 3)
```

**Conclusion** From the figure we can see that the empirical and theoretical distributions approximately agree again.

**(3) 3.12**

Obtain a random sample required of the *Exponential-Gamma* mixture and gain a histogram of it
```{r}
set.seed(777) #set a seed
n <- 1000
r <- 4
beta <- 2
lambda <- rgamma(n, r, beta)
rdm <- rexp(lambda)
hist(rdm, 
     freq = F, 
     main = "Exponential-Gamma Mixture",
     ylim = c(0, 1),
     col = "Peachpuff", 
     border = "Black",
     xlab = "mixture random numbers",
     las = 1)

```


**(4) 3.13**

**First**  we generate that mixture function sample and graph a histogram of it.

**Then**  we add the density curve over this figure 
```{r}
set.seed(888)  #set a seed
n <- 1000
r <- 4
beta <- 2
lambda <- rgamma(n, r, beta)
rdm <- rexp(lambda)
hist(rdm, 
     freq = F, 
     main = expression(f(x) == 64 / (2 + x) ^ 5),
     ylim = c(0, 1),
     col = "Peachpuff", 
     border = "Black",
     xlab = "mixture random numbers",
     las = 1)
lines(check <-  seq(0, 6, .01),
      64 / (2 + check) ^ 5,
      lwd = 3, 
      col = "Chocolate")
```

**Conclusion** From the figure we can see that the empirical and theoretical distributions approximately agree yet again.


## 3rd

## Question:

• For $n = 10^{4}, 2 × 10^{4}, 4 × 10^{4}, 6 × 10^{4}, 8 × 10^{4}$, apply the fast sorting algorithm to randomly permuted numbers of 1, . . . , n.

• Calculate computation time averaged over 100 simulations, denoted by $a_{n}$.

• Regress $a_{n}$ on $t_{n}$ := n log(n), and graphically show the results (scatter plot and regression line).

## Answer:

**_step1_**

first we construct a quicksorting function which could not only perform a quicksorting but count the time complexity during the process
```{r}
quick_sort<-function(sample){
  num<-length(sample)                          #acquire the sample for sorting
  if(num==0||num==1){return(sample)
  }else{
    first_one<-sample[1]                       #pick a number  
    rest_ones<-sample[-1]
    lower<-rest_ones[rest_ones < first_one]
    upper<-rest_ones[rest_ones >= first_one]   #sort the sample according to the quicksorting algorithm
    count <<-  count + num - 1                 #jot down the number of times of comparison i.e. time complexity
    return(c(quick_sort(lower),first_one,quick_sort(upper)))} #recursion until the sample is completely sorted
}
```

**_step2_** 

use a double for-loops to permute as well as compute the time complexity of 5 given quantity of numbers for 100 times each.
So to speak we use the first for loop to assign a certain quantity of a sample,
then we use the latter one to permute 100 times for that sample 
Finally these loops return an array whose elements are the 
average time complexity of the 5 quantity of numbers,i.e. $n=10^{4}, 2×10^{4}, 4×10^{4}, 6×10^{4}, 8×10^{4}$

```{r}
a <- numeric(5)                              #array "a" to contain the five results
total <- c(1e4, 2e4, 4e4, 6e4, 8e4)
for (j in 1:5) {
    total_count <- numeric(100)             
    #the array "total_count" is to contain the time complexity created by the following loop
    for (i in 1:100) {
        count <- 0
        random_sample <- sample(1:total[j])
        quick_sort(random_sample)
        total_count[i] <- count
    }
    a[j]<- sum(total_count) / 100
}
knitr::kable(a, 
             "pipe", 
             col.names = "average time complexity", 
             align = "l") #print array "a"
```

**_step3_**

regress $a_{n}$ and draw $t_{n}$=nlog(n) superimpose on it
```{r}
par(mar = c(5, 8, 4, 2), 
    yaxt = "s")
plot(total, 
     a,
     main = "Quicksort Computation Time",
     type = "b",
     xlab = "Number of Sample",
     ylab = "Computation Time",
     )                #regress array "a"
axis(side = 4)
line <- seq(1, 80000, 1)
lines(line, line*log(line), 
      lwd = 3, 
      col = "Chocolate") #draw nlog(n)
```

**conclusion** from the graphic we can find that the time complexity is O(nlog(n))


## Question:
 
consider antithetic approach on $\hat{\theta} = \int_{0}^{1} e^x\; dx$ ,Compute **Cov($e^{U}$, $e^{1-U}$) ** and
**Var($e^{U}$ + $e^{1-U}$) **, where $U \sim$Uniform(0, 1),compare variance of $\hat{\theta}$ estimated with antithetic variate with that of simple MC.

## Answer:

$$\begin{align*}
&Cov(e^{U}, e^{1-U}) = E(e^{U} - (e - 1))(e^{1-U} - (e - 1))=e-(e-1)^{2}=3e-e^{2}-1\\
&whereE(e^{U}) = E(e^{1-U}) = \int_{0}^{1} e^u\; du = e - 1\\
&Var(e^{u})=E(e^{u}-(e-1))^{2}=Ee^{2u}-(e-1)^{2}=\frac{1}{2}e^{2}-\frac{1}{2}-e^{2}+2e-1=2e-\frac{1}{2}e^{2}-\frac{3}{2}\\
&where  E(e^{2u})=\int_{0}^{1} e^{2u}\; du=\frac{1}{2}e^{2}-\frac{1}{2}\\
&Var(e^{u}+e^{1-u})=2Var(e^{u}) + 2Cov(e^{u},e^{1-u})=4e-e^{2}-3+6e-2e^{2}-2=10e-3e^{2}-5\\
&hence \frac{\hat{\theta}_{antithetic}}{\hat{\theta}_{simple}} = \frac{\frac{1}{2}×Var(e^{u}+e^{1-u})}{Var(e^{u})}\approx0.03232\\
\end{align*}$$

so the percent reduction being $1-0.03232=0.96768$

## Question:

Use a Monte Carlo simulation to estimate θ by the
antithetic variate approach and by the simple Monte Carlo method. Compute
an empirical estimate of the percent reduction in variance using the antithetic
variate. Compare the result with the theoretical value

## Answer:

**_step1_**

first construct a function of  simple method or using antithetic variate to achieve Monte Carlo simulation
```{r}
MC.simple_n_antithetic <- function(x, R = 1000, antithetic = FALSE){
    front_half <- runif(R/2)
    if(antithetic) back_half <- 1-front_half
    else           back_half <- runif(R/2)
    total_sample <- c(front_half, back_half)
    return(mean(x * exp(x * total_sample)))
}
```

**_step2_**

estimate $\theta$ by the antithetic approach and simple MOnte Carlo method using a random sample of 1000 numbers and compute the percent reduction in variance.

```{r}
MC_simple <-  MC_antithetic <- vector(length = 1000)
for (i in 1:1000) {
    MC_simple[i] <- MC.simple_n_antithetic(x = 1)
    MC_antithetic[i] <- MC.simple_n_antithetic(x = 1, antithetic = TRUE)
}
reduction <- 1 - var(MC_antithetic) / var(MC_simple)
knitr::kable(c(var(MC_simple), var(MC_antithetic), reduction), 
             "pipe", 
             col.names = "variance and reduction",
             align = "l")
```

**conclusion** we can see the empirical estimate of the reducing percentage and the theoretical one calculated in the former question approximately agree

## 4th

## Question:

Find two importance functions $f_{1}$ and $f_{2}$ that are supported on$(1 , \infty)$and are 'close' to 
$g(x) = \frac{x^{2}}{\sqrt{2\pi}}e^{-\frac{x^{2}}{2}},\quad x>1.$
Which of your two importance functions should produce the smaller variance in estimating
$\int_{1}^{\infty} \frac{x^{2}}{\sqrt{2\pi}}e^{-\frac{x^{2}}{2}} \; dx$
by importance sampling?Explian.

## Answer:

**_step1_**

In order to find the requested functions which are close to g(x),we could construct functions derived from $xe^{-\frac{x^{2}}{2}}\quad and \quad e^{-x}$ 
$$\begin{align*}
from\quad &\int_{1}^{\infty} c_{1}xe^{-\frac{x^{2}}{2}}\; dx=1\\
&c_{1}\left[\left. -e^{-\frac{x^{2}}{2}} \right|_{1}^{\infty}\right]=1\\
&c_{1}=\sqrt{e}\\
hence\quad&f_{1}(x)=\sqrt{e}xe^{-\frac{x^{2}}{2}}=xe^{-\frac{x^{2}-1}{2}}\\
from\quad &\int_{1}^{\infty} c_{2}e^{-x}\; dx=1\\
& c_{2}\left[\left. -e^{-x} \right|_{1}^{\infty}\right]=1\\
& c_{2}= e\\
hence\quad &f_{2}(x)=e^{1 - x}
\end{align*}$$

**_step2_**

Here we plot the densities of f1 and f2 with g on $(1, \infty)$ to see wether they are close enough
```{r}
x <- seq(1, 20, .01)
f1 <- sqrt(exp(1)) * x * exp(-x ^ 2 / 2)
f2 <- exp(1 - x)
g <- x ^ 2 * exp(-x ^ 2 / 2) / sqrt(2 * pi)
plot(x, g, type = "l", main = "f1, f2, with g", ylab = "", ylim = c(0, 0.6), lwd = 2)
lines(x, f1, lty = 2, lwd = 2)
lines(x, f2, lty = 3, lwd = 2)
legend("topright", legend = c("g", "f1", "f2"), lty = 1:3, lwd = 2)
```

**_step3_**

Here we plot the ratio g/f
```{r}
plot(x, g, type = "l", main = "g / f", ylab = "", ylim = c(0, 1), lwd = 2, lty = 2)
lines(x, g / f1, lty = 3, lwd = 2)
lines(x, g / f2, lty = 4, lwd = 2)
legend("topright", legend = c(0:2), lty = 2:4, lwd = 2)

```

**_step4_**

Use inverse transform method to estimate the integral and the corresponding variance with step4 being f1 part
```{r}
m <- 1e4
set.seed(123)
theta_hat <- v <- numeric(2)
# theta_hat and v to contain estimated theta and variance of f1 and f2
g <- function(x){
    x ^ 2 * exp(-x ^ 2 / 2)/sqrt(2 * pi) * (x > 1)
}
u <- runif(m)
x <- sqrt(1 - 2 * log(1 - u))
#inverse transformation to produce random number of f1
fg <- g(x) / (sqrt(exp(1)) * x * exp(-x ^ 2 / 2))
theta_hat[1] <- mean(fg)
v[1] <- var(fg)
```

**_step5_**

f2 part
```{r}
n <- 1e4
u <- runif(n)
y <- 1 - log(1 - u) 
#inverse transformation to produce random number of f2
fg <- g(y) / exp(1 - y)
theta_hat[2] <- mean(fg)
v[2] <- var(fg)
```

**_step6_**

print outcome of the estimate 
```{r}
rev <- data.frame(c("f1", "f2"), est <- theta_hat, va <- v)
knitr::kable(rev, "pipe", align = "l", col.names = c("importance function", "theta hat", "variance"))
```

**conclusion:** From the table in step 6 we could see that f1 produce the smaller variance,which is pretty reasonable considering the fact that f1 possesses a much more similar formation than f2 do.

## Question:

Obtain the stratified importance sampling estimate in Example 5.13 and compare it with the result of Example 5.10



## Answer:


**_step1_**

Obtain the stratified importance sampling estimate in Example 5.13

```{r}
M <- 1e3
u <- matrix(0, 200, 5)  
# 200 * 5 matrix to store replicates of random uniform distribution numbers
x <- matrix(0, 200, 5)  
# 200 * 5 matrix to store replicates of x from every strata generated by inverse transform method
fg <- matrix(0, 200, 5) 
#2 200 * 5 matrix to store replicates of gi / fi from every strata
T <- numeric(5)         
# T to store  theta j hat of every strata 
v <- numeric(5)
# v to store sigma j square of every strata
g <- function(x){
    exp(-x - log(1 + x ^ 2)) * (x > 0) *(x < 1)
}
p <- function(j){
    -log((5 - j) / 5 +j / 5 *exp(-1))
}
# function p() to calculate endpoints of every strata
for (j in 0:4) {
    u[ ,j + 1] <- runif(M/5, p(j), p(j + 1))
    x[ ,j + 1] <- -log(exp(-p(j)) - u[ ,j + 1] * (1 - exp(-1)) / 5)
    fg[ ,j + 1] <- g(x[ ,j + 1]) / (5 * exp(-x[ ,j + 1]) / (1 - exp(-1)))
    T[j + 1] <- mean(fg[ ,j + 1])
    v[j + 1] <- var(fg[ ,j + 1])
    j <- j + 1
}
# for loop to calculate theta j hat and sigma j square of each 5 strata respectively
(theta.hat <- sum(T))
(vari <- sum(v) / (M / 5))
```

**conclusion** From the result of 5.15 together with 5.10,we could see that both theta hat is 0.52. The estimate variance of stratified importance sampling estimate is much smaller than that of just importance function method,where the importance function approach produce a variance of 0.1 whereas the stratified importance sampling's variance is 3e-8

## 5th

## Question

Suppose that $X_{1},...,X_{n}$ are a random sample from a from a lognormal distribution with unknown parameters. Construct a 95% confidence interval for
the parameter µ. Use a Monte Carlo method to obtain an empirical estimate
of the confidence level.

## Answer

**_step1_**

$$\begin{align*}
&\log(X_{1}),...,\log(X_{n}) \sim N(\mu,\sigma^{2})\\
&\sqrt{n}\frac{\overline{\log(X)}-\mu}{sd(\log(X))} \sim t_{n-1}\\
&so\quad the \quad 95\% \quad confidence \quad interval \quad for \quad \mu \quad being:\\
&\left( \overline{\log{X}}-\frac{sd(\log(X))}{\sqrt{n}}t_{n-1}(0.025)\quad, \quad \overline{\log{X}}+\frac{sd(\log(X))}{\sqrt{n}}t_{n-1}(0.025) \right)\\
&where\quad t_{n-1}\quad denotes\quad the\quad Student\quad t \quad distribution \quad with \quad n-1 \quad degrees \quad of \quad freedom\\
\end{align*}$$

**_step2_**

**Use a Monte Carlo method to compute confidence level**

**_sub step1_**

Generating random numbers from lognormal distribution when $\mu = 0, \sigma^{2} = 1, i.e.X_{i} \sim LN(0, 1)$
```{r}
set.seed(234)
m <- 1e4  
# m denotes number of simulation
n <- 10
# n denotes sample size
data_generate <- function(m, n){

lndata <- matrix(0, m, n)
  for (i in 1:m) {
    lndata[i, ] <- rlnorm(n)
  }
 return(lndata)
} 
lndata <- data_generate(m, n)
# for loop to generate random sample
save(lndata,m,n, file = "4data.RData")
# save data
rm(list = ls())
gc()
#clear memory
```

**_sub step2_**

Obtain empirical confidence level when $\alpha = 0.05$

```{r}
alpha <- .05
load("4data.RData")
# load data
level_test <- function(m, n, lndata){
LCL <- UCL <- numeric(m)
for (i in 1:m) {
    LCL[i] <- mean(log(lndata[i, ])) - sd(log(lndata[i, ])) / sqrt(n) * qt(1 - alpha/2, df = n - 1)
    UCL[i]<- mean(log(lndata[i, ])) + sd(log(lndata[i, ])) / sqrt(n) * qt(1 - alpha/2, df = n - 1)
}
# for loop to calculate upper and lower confidence limits
outcome <- mean( (LCL <= 0) & (UCL >= 0) )
return(outcome)
}
outcome <- level_test(m, n, lndata)
# compute the mean to get the confidence level
save(outcome, file = "4outcome.RData")
# save data
rm(list = ls())
gc()
# clear memory
```

**_sub step3_**

Print result

```{r}
load("4outcome.RData")
report <- function(outcome){
paste(" Empirical estimate of the confidence level is:", outcome)
}
report(outcome)
write.table(outcome, file = "outcome4.txt")
rm(list = ls())
gc()
```
**conclusion:** The result is that 9495 intervals satisfied the restriction,so the empirical confidence level is 94.95% in this experiment.It's pretty closed to the theoretical value,which is 95% in this case.

## Question:

Refer to Example 6.16. Repeat the simulation, but also compute the F test
of equal variance, at significance level $\hat{\alpha} = 0.055$. Compare the power of the
Count Five test and F test for small, medium, and large sample sizes. (Recall
that the F test is not applicable for non-normal distributions.)

**_step1_**

Generate samples in small,medium,and large sizes

```{r}
set.seed(234)
n_small <- 20
n_medium <- 100
n_large <- 1000
# n_size for sample size of each sample
m <- 1e4
# m for number of simulations
sigma1 <- 1
sigma2 <- 1.5
generate <- function(x, n, m, sigma0){
    for (i in 1:m) {
        x[i, ] <- rnorm(n, 0, sigma0)
    }
    return(x)
}
x_small <- matrix(0, m, n_small)
x_medium <- matrix(0, m, n_medium)
x_large <- matrix(0, m, n_large)
y_small <- matrix(0, m, n_small)
y_medium <- matrix(0, m, n_medium)
y_large <- matrix(0, m, n_large)
x_small <- generate(x_small, n_small, m, sigma1)
x_medium <- generate(x_medium, n_medium, m, sigma1)
x_large <- generate(x_large, n_large, m, sigma1)
y_small <- generate(y_small, n_small, m, sigma2)
y_medium <- generate(y_medium, n_medium, m, sigma2)
y_large <- generate(y_large, n_large, m, sigma2)

#for loop to generate random numbers of small,medium and large sample for x and y
save(x_small, x_medium, x_large, y_small, y_medium, y_large, n_small, n_medium, n_large, m, file = "8xydata.RData")
# save data
rm(list = ls())
gc()
# clear memory
```

**_step2_**

simulate Example 6.16 and also compute F test of equal variance for each of these three samples

```{r}
alpha_hat <- 0.055
#pow_size to store p value from count5 test whereas pow.size stores p value of respective sample of F test
count5test <- function(x, y){
    X <- x - mean(x)
    Y <- y - mean(y)
    outx <- sum(X > max(Y)) + sum(X < min(Y))
    outy <- sum(Y > max(X)) + sum(Y < min(X))
    return(as.integer(max(c(outx, outy)) > 5))
}
# function to perform count5 test
load("8xydata.RData")
pow_small <- pow_medium <- pow_large <- pow.small <- pow.medium <- pow.large<- numeric(m)
for (i in 1:m) {
    pow_small[i] <- count5test(x_small[i, ], y_small[i, ])
    pow_medium[i] <- count5test(x_medium[i, ], y_medium[i, ])
    pow_large[i] <- count5test(x_large[i, ], y_large[i, ])
    pow.small[i] <- var.test(x_small[i, ], y_small[i, ])$p.value
    pow.medium[i] <- var.test(x_medium[i, ], y_medium[i, ])$p.value
    pow.large[i] <- var.test(x_large[i, ], y_large[i, ])$p.value
}
# for loop to compute p values of two tests with different sample sizes
pow_small_d <- mean(pow_small)
pow_medium_d <- mean(pow_medium)
pow_large_d <- mean(pow_large)
pow.small.d <- mean(pow.small <= alpha_hat)
pow.medium.d <- mean(pow.medium <= alpha_hat)
pow.large.d <- mean(pow.large <= alpha_hat)
# compute the mean to get power value of all tests
save(pow.small.d, pow.medium.d, pow.large.d, pow_small_d, pow_medium_d, pow_large_d,  file = "8powdata.RData")
rm(list = ls())
gc()
```

**_step3_**

Print result

```{r}
load("8powdata.RData")
outcome.8 <- data.frame(value.name <- c("count5small", "count5medium", "count5large", "F small", "F medium", "F large"), value <- c(pow_small_d, pow_medium_d, pow_large_d, pow.small.d, pow.medium.d, pow.large.d))
knitr::kable(outcome.8, "pipe", col.names = c("value name", "value"))
write.table(outcome.8, file = "outcome8.txt")
rm(list = ls())
gc()
```

**conclusion:** Table says the power of these two methods are approximately the same,with Count Five Test a little lower than F test of equal variance.

## Question:

If we obtain the powers for two methods under a particular simulation setting with 10,000 experiments: say, 0.651 for one method and 0.676 for another method. Can we say the powers are different at 0.05
level?What is the corresponding hypothesis test problem?Which test can we use? Z-test, two-sample t-test, paired-t test or McNemar test? Why?Please provide the least necessary information for hypothesis testing.

## Answer

The number of simulation of experiments is as large as 10000,so the powers of two corresponding methods is bond to be different.The hypothesis is$ H_{0}: power_{1}=power_{2} \quad vs \quad  H_{a}:power_{1} \neq power_{2}$.We could use Z test to compare p value in large sample distribution or use McNemar test to directly compare these two methods.To carry out this hypothesis testing,we will need to know the setting of simulation,i.e. the distribution of sample and sample size. 

## 6th

## Question:

Refer to the air-conditioning data set aircondit provided in the boot package. The 12 observations are the times in hours between failures of air-conditioning equipment [63, Example 1.1]:
3, 5, 7, 18, 43, 85, 91, 98,100, 130, 230, 487.
Assume that the times between failures follow an exponential model Exp(λ).
Obtain the MLE of the hazard rate λ and use bootstrap to estimate the bias
and standard error of the estimate.

## Answer:

**_step1_** import data from boot package

```{r}
data(aircondit, package = "boot")
aircondit
save(aircondit, file = "4data.RData")
rm(list = ls())
gc()
```

**_step2_**

**Obtain the formula of MLE of hazard rate**

$$\begin{align*}
&X_{i} \sim {\sf Exp}(\lambda)\\
&f_{i}(x_{i})=\lambda e^{- \lambda x} I_{(x>0)}\\
&Likelihood \quad function \quad being: \quad L(\lambda)=f(x_{1},...,x_{12})=\lambda ^{12}e^{- \lambda \sum_{i=1}^{12} x_{i}}\prod_{i=1}^{12}I_{(x_{i}>0)}\\
&We \quad can \quad derive \quad that \quad log-likelihood \quad function \quad is:\log(L(\lambda))=12\log(\lambda)-\lambda \sum_{i=1}^{12} x_{i} +\log(\prod_{i=1}^{12}I_{(x_{i}>0)})\\
&Then \quad from \quad \displaystyle \frac{\partial \log(L(\lambda))}{\partial \lambda}
=\frac{12}{\lambda}-\sum_{i=1}^{12} x_{i}=0 \quad We \quad obtain \quad that \quad the \quad MLE \quad of \quad \lambda \quad is: \quad
\hat{\lambda}=\frac{1}{\overline{x}}
\end{align*}$$

**_step3_** 

write a function called "boot" to estimate bias and standard error of bootstrap MLE estimation

```{r}
load("4data.RData")
set.seed(123)
outcome <- numeric(3)
#vector "outcome" to store estimate bias,standard error and sample standard error
boot <- function(aircondit){
    B <- 200
    lambda_star <- numeric(B)
    outcome <- numeric(3)
    lambda_hat <- 1 / mean(aircondit$hours)
    for (b in 1:B) {
        aircondit_star <- sample(aircondit$hours, replace = TRUE)
        lambda_star[b] <- 1 / mean(aircondit_star)
    }
    outcome[1] <- mean(lambda_star) - lambda_hat
    outcome[2] <- sd(lambda_star)
    outcome[3] <- sd(aircondit$hours)
    return(outcome)
}
# write "boot" function to calculate bias and standard error
outcome <- boot(aircondit)
save(outcome, file = "4outcome.RData")
rm(list = ls())
gc()
```

**_step4_**

write a fnuction called "present" to report result,i.e. bias and standard error:

```{r}
load("4outcome.RData")
present <- function(outcome){
    round(c(bias=outcome[1], se.boot=outcome[2], se.sample=outcome[3]), 6)
}
present(outcome)
rm(list = ls())
gc()
```

## Question:

Refer to Exercise 7.4. Compute 95% bootstrap confidence intervals for the
mean time between failures 1/λ by the standard normal, basic, percentile,
and BCa methods. Compare the intervals and explain why they may differ

## Answer:

**_step1_** 

import data from boot package

```{r}
data(aircondit, package = "boot")
aircondit
save(aircondit, file = "5data.RData")
rm(list = ls())
gc()
```

**_step2_**

Use boot function to conduct bootstrap:

```{r}
load("5data.RData")
library(boot)
set.seed(123)
outcome <- matrix(0, 4, 2)
m_time_f <- function(dat, ind){
    mean(dat[ind])
}
boot.obj <- boot(aircondit$hours,
                 R=2000,
                 statistic = m_time_f)
save(boot.obj, outcome, file = "5obj.RData")
rm(list = ls())
gc()
```

**_step3_**

write a function called "itc" to calculate certain confidence intervals and store them in a matrix

```{r}
load("5obj.RData")
set.seed(123)
itc <- function(boot.obj, outcome){
    ci <- boot.ci(boot.obj, type = c("basic", "norm", "perc", "bca"))
    outcome[1,]<-ci$norm[2:3]
    outcome[2,]<-ci$basic[4:5] 
    outcome[3,]<-ci$percent[4:5]
    outcome[4,]<-ci$bca[4:5]
    return(outcome)
}
outcome <- itc(boot.obj, outcome)
save(outcome, file = "5outcome.RData")
rm(list = ls())
gc()
```

**_step4_**

write a function called "present" to report result, i.e. four intervals

```{r}
load("5outcome.RData")
present <- function(outcome){
    cat("Normal interval:", "\n", "(", outcome[1,1], ",", outcome[1,2], ")", "\n")
    cat("Basic interval:", "\n", "(", outcome[2,1], ",", outcome[2,2], ")", "\n")
    cat("Percentile interval:", "\n", "(", outcome[3,1], ",", outcome[3,2], ")", "\n")
    cat("Bca interval:", "\n", "(", outcome[4,1], ",", outcome[4,2], ")", "\n")
}
present(outcome)
rm(list = ls())
gc()
```
**conclusion:** First of all, Bca interval should be the closest of all four intervals derived above for the fact that it has second order accuracy while others only have first or none. Normal interval and basic interval should be the least closest intervals because the sample follows exponential distribution instead of normal distribution,and basic interval is not as accurate as percentile one in most cases.

## Question:

Conduct a Monte Carlo study to estimate the coverage probabilities of the standard normal bootstrap confidence interval, the basic bootstrap confidence interval, and the percentile confidence interval. Sample from a normal population and check the empirical coverage rates for the sample mean. Find the proportion of times that the confidence intervals miss on the left, and the porportion of times that the confidence intervals miss on the right.

## Answer:

**_step1_**

First generate a random sample of 100k numbers which follows a standard normal distribution.

```{r}
set.seed(123)
sample0 <- rnorm(1e5)
#sample0 to store all of the random numbers needed in this project
save(sample0, file = "Adata.RData")
rm(list = ls())
gc()
```

**_step2_**

Use the sample derived above and function in "boot" to complete the requirements in 7.A,i.e.empirical coverage rates and the proportion of times that the confidence intervals miss on the either side by standard normal bootstrap confidence interval, the basic bootstrap confidence interval, and the percentile confidence interval with a function called "proces".

```{r}
load("Adata.RData")
library(boot)
boot.mean <- function(dat, ind){
    mean(dat[ind])
}
outcome <- matrix(0, 3, 3)
# 3 * 3 matrix "outcome" stores coverage rate,left and right side of interval in a row for three ways of confidence intervals calculation
set.seed(123)
proces <- function(sample0){
    outcome <- matrix(0, 3, 3)
    m <- 1e4
    mu <- 0
    count <- 0
    ci.norm <- ci.basic <- ci.perc <- matrix(NA,m,2)
    for (i in 1:m) {
        de <- boot(data = sample0[(count + 1):(count + 10)], statistic = boot.mean, R = 999)
        ci <- boot.ci(de, type = c("norm", "basic", "perc"))
        ci.norm[i,]<-ci$norm[2:3]
        ci.basic[i,]<-ci$basic[4:5] 
        ci.perc[i,]<-ci$percent[4:5]
        count <- count + 10
        }
    outcome[1, 1] <- mean(ci.norm[, 1] <= mu & ci.norm[, 2] >= mu)
    outcome[2, 1] <- mean(ci.basic[, 1]<=mu & ci.basic[, 2] >= mu)
    outcome[3, 1] <- mean(ci.perc[, 1] <= mu & ci.perc[, 2] >= mu)
    outcome[1, 2] <- mean(ci.norm[, 1] > mu)
    outcome[1, 3] <- mean(ci.norm[, 2] < mu)
    outcome[2, 2] <- mean(ci.basic[, 1] > mu)
    outcome[2, 3] <- mean(ci.basic[, 2] < mu)
    outcome[3, 2] <- mean(ci.perc[, 1] > mu)
    outcome[3, 3] <- mean(ci.perc[, 2] < mu)
    return(outcome)
}
outcome <- proces(sample0)
outcome
save(outcome, file = "poutcome.RData")
rm(list = ls())
gc()
```

**_step3_**

Results reporting with function "present"

```{r}
load("poutcome.RData")
present <- function(outcome){
    cat('norm =',
    outcome[1, 1], 
    '\n',
    'norm.left=',
    outcome[1, 2],
     '\n',
    'norm.right=',
    outcome[1, 3],
    '\n',
    'basic =',
    outcome[2, 1],
    '\n',
    'basic.left=',
    outcome[2, 2],
    '\n',
    'basic.right=',
    outcome[2, 3],
    '\n',
    'perc =',
    outcome[3, 1],
    '\n',
    'perc.left=',
    outcome[3, 2],
    '\n',
    'perc.right=',
    outcome[3, 3]
    )
}
present(outcome)
rm(list = ls())
gc()
```

## 7th

## Question: 

Refer to Exercise 7.7. Obtain the jackknife estimates of bias and standard
error of $\hat{\theta}$.

## Answer

**_step1_**

obtain data

```{r}
library(bootstrap)
save(scor, file = "scordata.RData")
rm(list = ls())
```

**_step2_**

Data analysis and result reporting.Use function "theta.es" to perform the estimation with data matrix.Function "jack_knife" is to estimate target estimator theta with jackknife method whose returning variable "outcome" jots down the bias and standard deviation by using this method.

```{r}
load("scordata.RData")
outcome <- numeric(2)
theta.es <- function(scor){
    return(eigen(cov(scor))$value[1]/sum(eigen(cov(scor))$value))
}
jack_knife <- function(scor){
    n <- nrow(scor)
    theta.jack <- numeric(n)
    theta.hat <- theta.es(scor)
    for (i in 1:n) {
        theta.jack[i] <- theta.es(scor[-i,])
    }
    outcome[1] <- (n-1)*(mean(theta.jack)-theta.hat)
    outcome[2]<- sqrt((n-1)*mean((theta.jack-mean(theta.jack))^2))
    return(outcome)
}
outcome <- jack_knife(scor)
save(outcome, file = "1outcome.RData")
rm(list = ls())
```

**_step3_**

Use function "report" to report the value from "outcome",i.e. bias and standard error of $\hat{\theta}$

```{r}
load("1outcome.RData")
report <- function(outcome){
    cat("bias:", outcome[1], "se:", outcome[2])
}
report(outcome)
rm(list = ls())
```
## Question:

In Example 7.18, leave-one-out (n-fold) cross validation was used to select the
best fitting model. Use leave-two-out cross validation to compare the models

## Answer:

In this question, one has to divide data into test sets of two partitions each,which will separate data of n entities into n/2 sets,Then the model fitting leaves out one test set in turn, so that the models are fitted n / 2 times. Noted that the number of data in this question is 53,so we will have to part the first 52 numbers into 26 sets and live out the last one. On with code part as follow:

**_step1_**

obtain data

```{r}
library(DAAG)
attach(ironslag)
save(ironslag, file = "2data.RData")
rm(list = ls())
```
**_step2_**

Use function "cv_2" to compute prediction error are obtained from the n/2-fold cross validation,save results in 
the matrix "outcome", which contains error of model k in its kth column. As stated above, we delete the last member of these two data sets to ensure the partition from going wrong.

```{r}
load("2data.RData")
n <- floor(length(magnetic) / 2) 
outcome <- matrix(0, 2 * n, 4)
# for cross validation 
# fit models on leave-two-out samples
cv_2 <- function(outcome, ironslag, n){
    mag <- magnetic[-53]
    che <- chemical[-53]
 for (k in 1:n) { 
    y <-mag[-c(2 * k - 1, 2 * k)]
    x <- che[-c(2 * k - 1, 2 * k)]
    J1 <- lm(y ~ x) 
    yhat1 <- J1$coef[1] + J1$coef[2] * che[c(2 * k - 1, 2 * k)] 
    outcome[c(2 * k - 1, 2 * k), 1] <-mag[c(2 * k - 1, 2 * k)] - yhat1
    J2 <- lm(y ~ x + I(x^2)) 
    yhat2 <- J2$coef[1] + J2$coef[2] * che[c(2 * k - 1, 2 * k)] + J2$coef[3] * che[c(2 * k - 1, 2 * k)]^2
    outcome[c(2 * k - 1, 2 * k), 2] <-mag[c(2 * k - 1, 2 * k)] - yhat2
    J3 <- lm(log(y) ~ x) 
    logyhat3 <- J3$coef[1] + J3$coef[2] * che[c(2 * k - 1, 2 * k)] 
    yhat3 <- exp(logyhat3)
    outcome[c(2 * k - 1, 2 * k), 3] <-mag[c(2 * k - 1, 2 * k)] - yhat3
    J4 <- lm(log(y) ~ log(x)) 
    logyhat4 <- J4$coef[1] + J4$coef[2] * log(che[c(2 * k - 1, 2 * k)]) 
    yhat4 <- exp(logyhat4) 
    outcome[c(2 * k - 1, 2 * k), 4] <-mag[c(2 * k - 1, 2 * k)] - yhat4
 }
    return(outcome)
}
outcome <- cv_2(outcome, ironslag, n)
save(outcome, file = "2outcome.RData")
rm(list = ls())
```

**_step3 and some conclusion_**

Use function "report" to report the result, i.e. the sum of error's square.
According to the prediction error criterion, Model 2, the quadratic model, is still the best fit for the data,as the result suggested in Example 7.18.We could notice that the error is a little smaller than that of Example 7.18,probably is because the removal of one element in the process of leave-two-out cross validation, such that the number of error produced is one less that 7.18 

```{r}
load("2outcome.RData")
report <- function(outcome){
cat("model1: ",
    mean(outcome[, 1]^2), "\t",
    "model2: ",
    mean(outcome[, 2]^2), "\t",
    "model3: ",
    mean(outcome[, 3]^2), "\t",
    "model4: ",
    mean(outcome[, 4]^2))
}
report(outcome)
rm(list = ls())
```


## Question:

Implement the bivariate Spearman rank correlation test for independence [255] as a permutation test. The Spearman rank correlation test statistic canbe obtained from function cor with method = "spearman". Compare the achieved significance level of the permutation test with the p-value reported
by cor.test on the same samples.

## Answer

**_step1_**

perform the spearman test for independence as required by question  on Sepal and Petal width, i.e. row 2 and 4 in data "iris" with function "aply_sp" to compute the test result of R replications and then calculate the p value.

```{r,  warning=FALSE, message=FALSE}
Spearman <- function(x, y) { 
    per_y <- sample(1:length(y))
    return(cor(x, y[per_y], method = "spearman"))
}
aply_sp <- function(x, y, R, outcome){
for (i in 1:R) {
    outcome[i] <- Spearman(x, y)
}
    return(outcome)
}
x<- as.numeric(iris[1:50, 2])
y<- as.numeric(iris[1:50, 4])
R <- 999
set.seed(12345)
outcome <- numeric(R)
outcome <- aply_sp(x, y, R, outcome)
t0<-(cor(x, y))
p_s <- mean(abs(c(t0, outcome)) >= abs(t0))
p_nor <- cor.test(x, y)$p.value
save(p_s, p_nor, file = "3data.RData")
rm(list = ls())
```

**_step2_**

Use function report to report result:

```{r}
load("3data.RData")
report <- function(p_s, p_nor){
    cat("significance level of the permutation test: ", p_s, "\n",
        "p value derived from cor.test on the same samples: ", p_nor)
}
report(p_s, p_nor)
rm(list = ls())
```

## 8th

## Question1 :

9.4 Implement a random walk Metropolis sampler for generating the standard Laplace distribution (see Exercise 3.2). For the increment, simulate from a normal distribution. Compare the chains generated when different variances are used for the proposal distribution. Also, compute the acceptance rates of each chain.

Use the Gelman-Rubin method to monitor convergence of the chain, and run the chain until it converges approximately to the target distribution according to $\hat{R}<1.2$

## Answer:

*Implement the random walk version of the Metropolis sampler to generate the target distribution standard Laplace distribution,*

$f(x)=\frac{1}{2}e^{-|x|},x \in R$

*using the proposal distribution $N(X_{t},\sigma^{2})$. In order to see the effect of different choices of* *variance of the proposal distribution, try repeating the simulation with different choices of σ.so*

$r(x_{t}, y)=\frac{f(Y)}{f(X_{t})}=\frac{\frac{1}{2}e^{-|y|}}{\frac{1}{2}e^{-|x_{t}|}}=e^{|x_{t}|-|y|}$

**_step1_**

Write down functions needed for this matter:
  "rw.Metropolis" to generate the Markov chain required by proposal distribution with sigma as its variance, x0 as initialize value and N as length of this chain.The return value of this function is a list of the chain and value "k" jotting down times of objection during the process
  
```{r}
rm(list = ls())
set.seed(123)
rw.Metropolis <- function(sigma, x0, N) { 
    x <- numeric(N) 
    x[1] <- x0 
    u <- runif(N) 
    k<-0 
    for (i in 2:N) { 
        y <- rnorm(1, x[i-1], sigma) 
        if (u[i] <= exp(abs(x[i - 1]) - abs(y))) 
            x[i] <- y 
        else { 
            x[i] <- x[i-1] 
            k<-k+1
        } 
        }
    return(list(x=x, k=k))
}
```

"cal_R_hat" to calculate the value of $\hat{R}$ with a parameter called "psi", which is a matrix containing cumulative sum of a Markov chain

```{r}
cal_R_hat <- function(psi) { 
        # psi[i,j] is the statistic psi(X[i,1:j]) 
        # for chain in i-th row of X
    psi <- as.matrix(psi) 
    n <- ncol(psi) 
    k <- nrow(psi)
    psi.means <- rowMeans(psi)     #row means 
    B <- n * var(psi.means)        #between variance est.
    psi.w <- apply(psi, 1, "var")  #within variances 
    W <- mean(psi.w)               #within est.
    v.hat <- W*(n-1)/n + (B/n) 
    r.hat <- v.hat / W
    return(r.hat)
    }
```

"list_2_vec",as the title suggests, is a function to convert a chain in list form into a chain contained in a vector.

```{r}
list_2_vec<- function(X, N){
    X_v <- numeric(N)
    X_v <- X$x
    return(X_v)
}
```

"Gelman", is a function to perform a Gelman-Rubin method to all of the Markov chain with different variances ,which are stored in the parameter "sigma", other parameters including  n for length of a chain, b for burned length and k for number of chains generated to perform a single monitor method.

```{r}
Gelman <- function(n, b, sigma, k){
    x_i <- c(-10, -5, 5, 10)
    X <- matrix(0, nrow=k, ncol=n)
    for (i in 1:k){ 
        rw <- rw.Metropolis(sigma, x_i[i], n) 
        X[i, ] <- list_2_vec(rw, n)
    }
    psi <- t(apply(X, 1, cumsum)) 
    for (i in 1:nrow(psi)) 
        psi[i,] <- psi[i,] / (1:ncol(psi))
    print(cal_R_hat(psi))
    for (i in 1:k) 
        plot(psi[i, (b+1):n], 
            type="l", 
            xlab=i, 
            ylab=bquote(psi))
    rhat <- rep(0, n) 
    for (j in (b+1):n) 
        rhat[j] <- cal_R_hat(psi[,1:j]) 
    plot(rhat[(b+1):n], type="l", main = paste("sigma: ",sigma, ", n: ", n, ", burn: ", b), xlab="n - burn", ylab="R")
    abline(h=1.2, lty=2)
}
```

**_step2_**
generating the standard Laplace distribution using random walk Metropolis samplers with normal distributions whose variances vary from 0.5, 2, 8 and 16. Then compute the acceptance rates of each chain.

*Four chains are generated for different variances σ2 of the proposal distribution.*

```{r}
N <- 2000 
sigma <- c(.5, 2, 8, 16)
x0 <- 25 
rw1 <- rw.Metropolis(sigma[1], x0, N) 
rw2 <- rw.Metropolis(sigma[2], x0, N) 
rw3 <- rw.Metropolis(sigma[3], x0, N) 
rw4 <- rw.Metropolis(sigma[4], x0, N)
#number of candidate points rejected
print(c(rw1$k, rw2$k, rw3$k, rw4$k) / N)
rw <- cbind(rw1$x, rw2$x, rw3$x, rw4$x)
for (j in 1:4) {
plot((rw)[,j], 
     type="l",
     xlab=bquote(sigma == .(round(sigma[j],3))),
     ylab="X", 
     ylim=range(rw[,j]))
}
```

**conclusion :** 

  From the outcome of chains generated when different variances are used for the proposal distribution, we could find that in plot 1,which derives from the proposal distribution with the variance of 0.5, is converging very slowly and requires a much longer burn-in period.In the second plot (σ = 2) the chain is mixing well and converging to the target distribution after a short burn-in period of about 100.In the third and fourth plot, where σ = 8 and 16, the ratios r(Xt,Y) are smaller and most of the candidate points are rejected,especially the last one. The fourth chain converges, but it is inefficient.
  So to speak When variance of the increment is too large, most of the candidate points are rejected and the algorithm is very inefficient. If the variance of the increment is too small, the candidate points are almost all accepted, so the random walk Metropolis generates a chain that is almost like a true random walk, which is also inefficient.
   From the selection criterion of acceptance rate in 0.15 to 0.5,we should choose the second chain, which is the only chain whose rate falls in that region. 


**_step3_** 

Use the Gelman-Rubin method to monitor convergence of the chain, and run the chain until it converges approximately to the target distribution according to $\hat{R}<1.2$ Use the length of 20k, 2k, 2k and 1.5k for the total length of each chain and 5k, 500, 200 and 100 to burn respectively. Repeat this process with each variance used previously.

```{r}
k <- 4
n <- c(20000, 2000, 2000, 1500)
b <- c(5000, 500, 200, 100)
for (i in 1:k) {
    Gelman(n[i], b[i], sigma[i], k)
}
rm(list=ls())
gc()
```

**conclusion plus:** From the result we could see that the four type of chains converge after 14+5=19k,1+2=3k,800+200=1k, and 400+100=500 iterations according to $\hat{R}<1.2$.


## Question 2:

9.7 Implement a Gibbs sampler to generate a bivariate normal chain $(X_{t}, Y_{t})$ with zero means, unit standard deviations, and correlation 0.9. Plot the generated sample after discarding a suitable burn-in sample. Fit a simple linear regression model Y = β0 + β1X to the sample and check the residuals of the model for normality and constant variance.

    Use the Gelman-Rubin method to monitor convergence of the chain, and run the chain until it converges approximately to the target distribution according to $\hat{R}<1.2$

## Answer:

$(X_{1},X_{2}) \sim N(0, 0, 1, 1, 0.9)$
The conditional densities of a bivariate normal distribution are univariate normal
with parameters
$$\begin{align*}
&E[X_{2}|x_{1}]=\mu_{1}+\rho\frac{\sigma_{2}}{\sigma_{1}}(x_{1}-\mu_{1})=0.9x_{1}\\
&Var(X_{2|x_{1}})=(1-\rho^{2})\sigma_{2}^{2}=0.19\\
\end{align*}$$
and the chain is generated by sampling from

$$\begin{align*}
&f(x_{1}|x_{2}) \sim N(\mu_{1}+\frac{\rho\sigma_{1}}{\sigma_{2}}(x_{2}-\mu_{2}),(1-\rho^{2})\sigma_{1}^{2}) \sim N(0.9x_{2},0.19)\\
&f(x_{2}|x_{1}) \sim N(\mu_{2}+\frac{\rho\sigma_{2}}{\sigma_{1}}(x_{1}-\mu_{1}),(1-\rho^{2})\sigma_{2}^{2}) \sim N(0.9x_{1},0.19)\\
\end{align*}$$


**_step1_**

Write down functions needed for this matter:

function "MC_bnorm" to generate a bivariate normal chain using Gibbs sampler method, the initialized value is passed through parameter mu_i1 and mu_i2 to X and Y,N stands for the length of chain and X jotting down the matrix storing the chain.
```{r}
rm(list = ls())
set.seed(123)
MC_bnorm <- function(X, N, mu_i1, mu_i2){
    rho <- .9
    #correlation
    mu1 <- mu2 <- 0
    sigma1 <-sigma2 <- 1 
    s1 <- sqrt(1-rho^2)*sigma1
    s2 <- sqrt(1-rho^2)*sigma2
    X[1, ] <- c(mu_i1, mu_i2) #initialize
    for (i in 2:N) { 
        y <- X[i-1, 2] 
        m1 <- mu1 + rho * (y - mu2) * sigma1/sigma2 
        X[i, 1] <- rnorm(1, m1, s1) 
        x <- X[i, 1] 
        m2 <- mu2 + rho * (x - mu1) * sigma2/sigma1 
        X[i, 2] <- rnorm(1, m2, s2)
        }
    return(X)
}
```

function "cal_R_hat" to calculate the value of $\hat{R}$ with a parameter called "psi", which is a matrix containing cumulative sum of a Markov chain

```{r}
cal_R_hat <- function(psi) { 
        # psi[i,j] is the statistic psi(X[i,1:j]) 
        # for chain in i-th row of X
    psi <- as.matrix(psi) 
    n <- ncol(psi) 
    k <- nrow(psi)
    psi.means <- rowMeans(psi)     #row means 
    B <- n * var(psi.means)        #between variance est.
    psi.w <- apply(psi, 1, "var")  #within variances 
    W <- mean(psi.w)               #within est.
    v.hat <- W*(n-1)/n + (B/n) 
    r.hat <- v.hat / W
    return(r.hat)
}
```

function "Gelman" to perform a Gelman-Rubin method.Its parameter n stands for the length of the chain, b for the length to burn, k for times of replicate. And because the chain generated in this problem actually contains two arrays, which stands for the chain of X and Y.Hence if the parameter "margin" is T or default value, this function perform Gelman-Rubin method on X and likewise if the value of margin is F then this function erform Gelman-Rubin method on Y. 

```{r}
Gelman <- function(n, b, k, margin = T){
    x_i <- c(0, 0, 5, 5, -5, -5, 10, 10)
    X <- matrix(0, nrow = k, ncol=n)
    for (i in 1:k){ 
            tmp <- matrix(0, N, 2)
            tmp <- MC_bnorm(tmp, N, x_i[2 * i - 1], x_i[2 * i])
        if(margin)
            {X[i, ] <- tmp[, 1]}
        else
            {X[i, ] <- tmp[, 2]}
        }
    psi <- t(apply(X, 1, cumsum)) 
    for (i in 1:nrow(psi)) 
        psi[i,] <- psi[i,] / (1:ncol(psi))
    print(cal_R_hat(psi))
    for (i in 1:k) 
        plot(psi[i, (b+1):n], 
             type="l", 
             xlab=i, 
             ylab=bquote(psi))
    rhat <- rep(0, n) 
    for (j in (b+1):n) 
        rhat[j] <- cal_R_hat(psi[,1:j]) 
    plot(rhat[(b+1):n], type="l", xlab="n - burn", ylab="R")
    abline(h=1.2, lty=2)
}
```

**_step2_**

generate a bivariate normal chain $(X_{t}, Y_{t})$ with zero means, unit standard deviations, and correlation 0.9. Plot the generated sample after discarding burn-in samples of 100.

```{r}
N <- 5000 #length of chain 
 burn <- 100 #burn-in length
 X <- matrix(0, N, 2) #the chain, a bivariate sample 
 X <- MC_bnorm(X, N, 0, 0)
 b <- burn + 1
 X <- X[b:N, ]
X_f <- as.data.frame(X)
names(X_f)[1] <- "x"
names(X_f)[2] <- "y"
X_l <- lm(y ~ x, X_f)
plot(X, 
     main="Generated sample after burning out 0.1k", 
     cex=.5, 
     xlab=bquote(X), 
     ylab=bquote(Y), 
     ylim=range(X[,2]))
abline(X_l, col = "Chocolate")
```

**_step3_**

Fit a simple linear regression model Y = β0 + β1X to the sample and check the residuals of the model for normality and constant variance.

```{r}
(X_reg <- summary(X_l))
```
From the result we could see that the Residual standard error is 0.443, and constant variance is $0.006346^{2}$

**_step4_**

Use the Gelman-Rubin method to monitor convergence of the chain, and run the chain until it converges approximately to the target distribution according to $\hat{R}<1.2$

```{r}
k <- 4
Gelman(N, burn, k, T)
Gelman(N, burn, k, F)
rm(list=ls())
gc()
```


**conclusion plus: ** From the result we could see that X converges after 600 while Y converges after 2500 iterations.So this bivariate normal chain $(X_{t}, Y_{t})$ generated with Gibbs sampler method should converge after 2500 iterations.

## 9th

## Question1

From $M=a_{M}+\alpha X+e_{M},\quad Y=a_{Y}+\beta M+ \gamma X + e_{Y}, \quad e_{M},e_{Y} \quad iid \quad \sim N(0,1)$ ,test the performance of these three method: $(1) \quad \alpha = 0 \quad (2) \beta = 0 \quad (3) \alpha = \beta =0$  under the model of $(1) \quad \alpha=\beta=0, \quad (2) \quad \alpha=0 \quad \beta=1 \quad (3) \quad \alpha=1 \quad beta=0$while$\gamma=1$
     





## Answer1

Use R package "mediation" in three setted models:

**_step0_**

write a function to perform objective test: 

```{r}
test <- function(alpha, beta, gamma, N, X, em, ey){
    a_m <- 1
    a_y <- 1
    M <- a_m + alpha * X + em
    Y <- a_y + beta * M +gamma * X + ey
    data.one <- data.frame(X, M, Y)
    X.M <- lm(M ~ X, data = data.one)
    X_M.Y <- lm(Y ~ X + M, data = data.one)
    cont <- mediate(X.M, X_M.Y, treat = "X", mediator = "M", sims = 100, boot = T)
    tmp1 <- summary(X.M)
    tmp2 <- summary(X_M.Y)
    tmp3 <- summary(cont)
    print(tmp1) #print the result regression of x to M and X,M to Y as well as result of using mediation package
    print(tmp2)
    print(tmp3)
    plot(cont)
}

```

**_step1_**

**model 1: **  $\alpha=\beta=0 \quad \gamma=1$    
**model 2: **$\alpha=0 \quad \beta=1 \quad \gamma=1$     
**model 3: **$\alpha=1 \quad \beta=0 \quad \gamma=1$

```{r,  warning=FALSE, message=FALSE}
library(mediation)
set.seed(123)
gamma <- 1
N <- 1e3
em <- rnorm(N)
ey <- rnorm(N)
X <- rpois(N, 1)
for (alpha in 0:1) {
    for (beta in 0:1) {
        if((alpha ==1) && (beta ==1)) break
        test(alpha, beta, gamma, N, X, em, ey)
    }
}
rm(list=ls())
gc()
```



**conclusion1** From the result of linear regression and mediation, we could see that alpha equals 0 significantly in the first linear regression part, but beta doesn't under significant level of 0.05,whose p value is 0.00627.So in this model alpha=0 while beta!=0.Under the result of ACME of function "mediate",which significantly equals to 0 under the p value of 0.42,indicating that alpha * beta =0.   
So to speak the first type of error of test of alpha * beta is 0.42 while type I error in test No 1 is 0.35, in test No2 is 0.00627,in test No3 is 0.35,So these three test can't control type I error of the original test.







**conclusion2** From the result of linear regression and mediation, we could see that alpha equals 0 significantly in the first linear regression part, and beta equals 1 significantly.So in this model alpha=0 while beta=1.Under the result of ACME of function "mediate",which significantly equals to 0 under the p value of 0.42,indicating that alpha * beta =0.   
So to speak the first type of error of test of alpha * beta is 0.42 while type I error in test No 1 is 0.354, in test No3 is 0.354 too,So these three test can't control type I error of the original test.






**conclusion3** From the result of linear regression and mediation, we could see that alpha equals 1 significantly in the first linear regression part, but beta doesn't under significant level of 0.05,whose p value is 0.00627.So in this model alpha=1 while beta!=0.Under the result of ACME of function "mediate",which doesn't significantly equals to 0 either,indicating that alpha * beta !=0.   
So to speak the first type of error of test of alpha * beta is 0 while type I error in test No2 is 0.00627,in test No3 is 0.00627,So these three test can't control type I error of the original test.

## Question2

**From** $P(Y = 1|X_{1}, X_{2}, X_{3})=expit(a+b_{1}x_{1}+b_{2}x_{2}+b_{3}x_{3})$,   
**where** $X_{1} \sim P(1)$, $X_{2} \sim Exp(1)$, $X_{3} \sim B(1,0.5)$   
**(1) Write down a function to calculate alpha given** $N,b_{1},b_{2},b_{3},f_{0}$   
**(2) Call the function wrote above with parameter** $N=10^{6}, \quad b_{1}=0, \quad b_{2}=1, \quad b_{3}=-1,\quad f_{0}=0.1,0.01,0.001,0.0001$   
**(3)Plot** $f_{0}$ **vs** $alpha$

## Answer2

**_step1_**

```{r}
logi.reg <- function(N, b1, b2, b3, f0){
    x1 <- rpois(N, 1)
    x2 <- rexp(N, 1)
    x3 <- rbinom(N, 1, .5)
    g <- function(alpha){
        tmp <- exp(-alpha - b1 * x1 - b2 * x2 - b3 * x3) 
        p <- 1/(1 + tmp)
        mean(p) - f0
    }
    solution <- uniroot(g,c(-20,0))
    return(solution$root)
}
```

**_step2_**

```{r}
root<- numeric(4)
f<- numeric(4)
f
for (i in 1:4 ) {
    f[i] <- 1 / 10 ^ i
    root[i] <- logi.reg(1e6, 0, 1, -1, f[i])
    paste("when f0 is: ", f, "root is: ", root[i], "\n")
}
```

**_step3_**
```{r}
plot(-log(f, base = 10), root)
rm(list=ls())
gc()
```


**conclusion** We could see that logf0 and alpha are approximately linear

## 10th

## Question:

handwritten homework about EM algorithm

## Answer:


**_step1_**


$For \quad X_{1},...,X_{n} \quad iid \quad \sim Exp(\lambda)$
So the likelihood function is :$L_{O}(\lambda)=\prod_{i = 1}^{n} P_{\lambda}(u_{i} \le x_{i} \le v_{i})=\prod_{i = 1}^{n} (e^{- \lambda u_{i}}-e^{- \lambda v_{i}})$      
$l_{O}(\lambda)=\sum_{i = 1}^{n} \log(e^{- \lambda u_{i}}-e^{- \lambda v_{i}})$     
From $\displaystyle \frac{\partial l_{O}(\lambda)}{\partial \lambda}=\sum_{i = 1}^{n} \frac{v_{i}e^{- \lambda v_{i}} - u_{i}e^{- \lambda u_{i}}}{e^{- \lambda u_{i}}-e^{- \lambda v_{i}}}=0$     
we could derive Observed data likelihood estimation by solving this nonlinear equation afterwards using r package "BB"
     
Complete data likelihood:$L_{C}(\lambda)=\prod_{i = 1}^{n} \lambda e^{- \lambda x_{i}}= \lambda^{n}e^{- \lambda \sum_{i = 1}^{n} x_{i}}$
So$l_{C}(\lambda)=n \log(\lambda)-\lambda \sum_{i = 1}^{n} x_{i}$     
So the expectation being:$E(l_{C}(\lambda|u_{i}, v_{i}))=n \log(\lambda)- \lambda \sum_{i = 1}^{n} E(x_{i}|u_{i},v_{i})$

From
$P(x_{i}|u_{i}, v_{i})=\frac{P(X_{i} \le x_{i})}{P(u_{i} \le X_{i} \le v_{i})}$

We could derive that $f(x_{i}|u_{i},v_{i})= \frac{\lambda e^{- \lambda x_{i}}}{e^{- \lambda u_{i}}-e^{- \lambda v_{i}}}$

So $E(x_{i}|u_{i},v_{i})=\int_{u_{i}}^{v_{i}} x_{i}f(x_{i}|u_{i},v_{i}) \; dx_{i}=\int_{u_{i}}^{v_{i}} x_{i} \frac{\lambda e^{- \lambda x_{i}}}{e^{- \lambda u_{i}}-e^{- \lambda v_{i}}} \; dx=\frac{u_{i}e^{- \lambda u_{i}}-v_{i}e^{- \lambda v_{i}}}{e^{- \lambda u_{i}}-e^{- \lambda v_{i}}}+ \frac{1}{\lambda}$    
Take this result back to the conditional expectation above:
$E(l_{C}(\lambda|u_{i}, v_{i}))=n \log(\lambda)- \lambda (\sum_{i = 1}^{n} \frac{u_{i}e^{- \lambda u_{i}}-v_{i}e^{- \lambda v_{i}}}{e^{- \lambda u_{i}}-e^{- \lambda v_{i}}}+ \frac{n}{\lambda})$   
Now we turn to step M,which is to calculate the value of $\lambda$ when the expectation hit its maximum scale:    
$\displaystyle \frac{\partial E(l_{C}(\lambda|u_{i}, v_{i}))}{\partial \lambda}= \frac{n}{\lambda}-\sum_{i = 1}^{n} \frac{u_{i}e^{- \lambda u_{i}}-v_{i}e^{- \lambda v_{i}}}{e^{- \lambda u_{i}}-e^{- \lambda v_{i}}}-\lambda \sum_{i = 1}^{n}\displaystyle \frac{\partial }{\partial \lambda}  \frac{u_{i}e^{- \lambda u_{i}}-v_{i}e^{- \lambda v_{i}}}{e^{- \lambda u_{i}}-e^{- \lambda v_{i}}}$     
$\hat{\lambda}_{1}=\frac{n}{ \frac{u_{i}e^{- \lambda_{0} u_{i}}-v_{i}e^{- \lambda_{0} v_{i}}}{e^{- \lambda_{0} u_{i}}-e^{- \lambda_{0} v_{i}}}+\frac{n}{\hat{\lambda_{0}}}}$     
Such that the recursive formula being:    
$\hat{\lambda}_{k}=\frac{n}{ \frac{u_{i}e^{- \lambda_{k-1} u_{i}}-v_{i}e^{- \lambda_{k-1} v_{i}}}{e^{- \lambda_{k-1} u_{i}}-e^{- \lambda_{k-1} v_{i}}}+\frac{n}{\hat{\lambda_{k-1}}}},k=1,2,...$     
If this recursive formula converges, than $\hat{\lambda}_{\infty}=\frac{n}{ \frac{u_{i}e^{- \lambda_{\infty} u_{i}}-v_{i}e^{- \lambda_{\infty} v_{i}}}{e^{- \lambda_{\infty} u_{i}}-e^{- \lambda_{\infty} v_{i}}}+\frac{n}{\hat{\lambda_{\infty}}}}$    
Using $|\hat{\lambda_{k}}-\hat{\lambda_{k-1}}|=|\frac{n}{ \frac{u_{i}e^{- \lambda_{k-1} u_{i}}-v_{i}e^{- \lambda_{k-1} v_{i}}}{e^{- \lambda_{k-1} u_{i}}-e^{- \lambda_{k-1} v_{i}}}+\frac{n}{\hat{\lambda_{k-1}}}}-\frac{n}{ \frac{u_{i}e^{- \lambda_{k-2} u_{i}}-v_{i}e^{- \lambda_{k-2} v_{i}}}{e^{- \lambda_{k-2} u_{i}}-e^{- \lambda_{k-2} v_{i}}}+\frac{n}{\hat{\lambda_{k-2}}}}|$    
Using contraction mapping we could prove that lambda hat converges.


**_step2_**

write function "EM_way" to perform EM method on the given data and use r package to get the estimate value of lambda from solving the equation of likelihood function of observed data

```{r}
EM_way <- function(interval, lam_0){
    n <- nrow(interval)
    summ <- 0
    for (i in 1:n) {
        summ <-summ + (interval[i, 1] * exp(-lam_0 * interval[i, 1])-interval[i, 2] * exp(-lam_0 * interval[i, 2]))/(exp(-lam_0 * interval[i, 1])-exp(-lam_0 * interval[i, 2])) 
    }
   lam_1 <- n / (summ + n/lam_0)
   if(abs(lam_0 - lam_1) <= 1e-10) return(lam_1)
   else return(EM_way(interval, lam_1))
}
ob_way <- function(lam_o){
    for (i in 1:10) {
    ob<-ob+(interval[i, 2]*exp(-interval[i, 2]*lam_o)-interval[i, 1]*exp(-interval[i, 1]*lam_o))/(exp(-interval[i, 2]*lam_o)-exp(-interval[i, 1]*lam_o))
    }
  f<-ob

  f
}

```

**_step3_**

install package "BB" to solve nonlinear equation

```{r, message=FALSE, warning=FALSE}
library(BB)
ob <- 0
lam_0 <- 1
interval <- matrix(c(11, 8, 27, 13, 16, 0, 23, 10, 24, 2, 12, 9, 28, 14, 17, 1, 24, 11, 25, 3), 10, 2)
paste("lambda hat derived from EM method is:", EM_way(interval, lam_0))
starl<-c(0.7)
result=dfsane(starl,ob_way,control = list(maxit=2500,trace=FALSE))
paste("lambda hat derived from observed data is : ", result$par)
rm(list=ls())
```
**conclusion:** From the numerical solution we could see that the result of this two methods are approximately same.

## Question:
Why do you need to use unlist() to convert a list to an atomic vector? Why doesn’t as.vector() work?

## Answer:

As the book suggested at the beginning of 2.1:"Vectors come in two flavours: atomic vectors and lists."List is a type of vector already,Using as.vector on a list won't convert it into a atomic vector,Only to let the list remain as it is.

```{r}
test.l.1 <-list(1:4, "a", T, list())
test.l.2 <- as.vector(test.l.1)
test.l.3 <- unlist(test.l.1)
str(test.l.1)
str(test.l.2)
str(test.l.3)
rm(list=ls())
```

## Question:

Why is 1 == "1" true? Why is -1 < FALSE true? Why is "one" < 2 false?

## Answer:

  Coercion often happens automatically. Most mathematical functions (+, log, abs, etc.) will coerce to a double or integer, and most logical
operations (&, |, any, etc) will coerce to a logical.    
  The hierarchy for coercion is: logical < integer < numeric < character.    
  So in the first case where 1 is compared with"1", the double atomic vector is coerced to character atomic vector and no doubt "1" == "1" is true.   
  In the second case FALSE is coerced to double atomic vector 0 and -1 < 0 returns true apparently.   
  However when "one" is compared with 2 in the third case, 2 is coerced to character atomic vector "2" and the expression is comparing "one" with "2" actually,which is the comparison of strings.I looked up and find out the comparing rules of R may varies:"Comparison of strings in character vectors is lexicographic within the strings using the collating sequence of the locale in use: see locales. The collating sequence of locales such as en_US is normally different from C (which should use ASCII) and can be surprising.".So in this case "one" >= "2",i.e."o" > "2" could be the comparison of the ASCII.

## Question:

What does dim() return when applied to a vector?

## Answer:

returns NULL

```{r}
dim(1:10)
dim(list(1, "a", T))
rm(list=ls())
```

## Question:

If is.matrix(x) is TRUE, what will is.array(x) return?

## Answer:
It would return TRUE      
As it is written in the description of as.matrix() and as.array() function:"is.matrix returns TRUE if x is a vector and has a "dim" attribute of length 2 and FALSE otherwise."    
"An array in R can have one, two or more dimensions. It is simply a vector which is stored with additional attributes giving the dimensions (attribute "dim") and optionally names for those dimensions (attribute "dimnames").A two-dimensional array is the same thing as a matrix."    
So a matrix is also an array,is.array(x) would return TRUE if is.matrix(x) returns TRUE in the first place.


## Question:

What attributes does a data frame possess?

## Answer:

A data frame shares properties of both the matrix and the list.So similar to matrices, data frames will have a dimension attribute. In addition, data frames can also have additional attributes such as row names, column names, and comments.

```{r}
df <- data.frame(x = 1:4, y = c("a", "b", "c", "d"), stringAsFactors = FALSE)
str(df)
attributes(df)
rm(list=ls())
```


## Question:

What does as.matrix() do when applied to a data frame with columns of different types?


## Answer:

as.matrix() convert a data frame with columns of different types to a matrix whose elements' type got coerced to the most "flexible" type as the rank mentioned above: logical < integer < numeric < character.

```{r}
df <- data.frame(x = 1:4, y = c("a", "b", "c", "d"), z <- c(T, T, F, T), stringAsFactors = FALSE)
test <- as.matrix(df)
typeof(test)
str(test)
rm(list=ls())
```
## Question:

Can you have a data frame with 0 rows? What about 0 columns?

## Answer:

We could make a data frame with 0 rows by data.frame() function 

```{r}
test=data.frame(col1=character(0),col2=numeric(0),col3=logical(0))
str(test)
rm(list=ls())
```

or transform a matrix with no rows to a data frame 

```{r}
test=matrix(data=NA,nrow = 0,ncol = 3)
test=as.data.frame(test)
colnames(test)=c("col1","col2","col3")
str(test)
rm(list=ls())
```

We could also make a 0 columns data frame with data.frame()

```{r}
df.empty <- data.frame(row1="",row2="",row3="")[-1,]
str(df.empty)
rm(list=ls())
```

## 11th

## Question1:

The function below scales a vector so it falls in the range [0, 1]. How would you apply it to every column of a data frame? How would you apply it to every numeric column in a data frame?\
scale01 \<- function(x) {\
rng \<- range(x, na.rm = TRUE) (x - rng[1]) / (rng[2] - rng[1])\
}

## Answer:

Since data frames are also lists, lapply() is also useful when one want to do something to each column of a data frame.

***step1***

```{r}
library(knitr)
scale01 <- function(x) {       
  rng <- range(x, na.rm = TRUE) 
  (x - rng[1]) / (rng[2] - rng[1])       
}
df <- data.frame(lapply(mtcars,function(x) scale01(x) ))
df
rm(list=ls())
```

***step2***

Noted that this function needs numeric input,we could use an if clause to only process numeric columns and else to return non-numeric columns as it is.

```{r}
scale01 <- function(x) {       
  rng <- range(x, na.rm = TRUE) 
  (x - rng[1]) / (rng[2] - rng[1])       
}
df <- data.frame(lapply(iris, function(x) if (is.numeric(x)) scale01(x) else x))
df
rm(list=ls())
```

## Question2:

1.  Use vapply() to:\

```{=html}
<!-- -->
```
a)  Compute the standard deviation of every column in a numeric data frame.\
b)  Compute the standard deviation of every numeric column in a mixed data frame. (Hint: you'll need to use vapply() twice.)

## Answer:

***step1***

for pure numeric data frame we choose mtcars:

```{r}
library(knitr)
data <- vapply(mtcars, 
       sd, 
       numeric(1))
kable(data, 
      "pipe",
      align = "l")
rm(list=ls())
```

***step2***

Using the first vapply inside to get the numeric column of a mixed data frame and use the other vapply to do the calculation:

```{r}
library(knitr)
data <- vapply(iris[vapply(iris, is.numeric, logical(1))],
       sd, 
       numeric(1))
kable(data, 
      "pipe",
      align = "l")
rm(list=ls())
```

## Question3:

Implement a Gibbs sampler to generate a bivariate normal chain (Xt, Yt) with zero means, unit standard deviations, and correlation 0.9.\
• Write an Rcpp function.\
• Compare the corresponding generated random numbers with pure R language using the function "qqplot".\
• Compare the computation time of the two functions with the function "microbenchmark".

## Answer:

install package {RVAideMemoire} to use function "mqqnorm" for drawing QQ-plots to assess multivariate normality generated by Gibbs methods of R and Cpp.

***step1***

```{r}
library(Rcpp)
cppFunction(code='NumericMatrix GibbsC(int N, double mu_i1, double mu_i2){
    NumericMatrix mat(N, 2);
    double rho = .9;
    double mu1 = 0, mu2 = 0;
    double sigma1 = 1, sigma2 = 1; 
    double s1 = sqrt(1 - rho * rho) * sigma1;
    double s2 = sqrt(1 - rho * rho) * sigma2;
    double x =0, y = 0, m1 = 0, m2 = 0;
    NumericVector tmp(1);
    mat(0,0) = mu_i1;
    mat(0,1) = mu_i2;
    for (int i=1; i < N; ++i) { 
        y = mat(i-1, 1); 
        m1 = mu1 + rho * (y - mu2) * sigma1/sigma2; 
        tmp = rnorm(1, m1, s1);
        mat(i, 0) = tmp[0]; 
        x = mat(i, 0); 
        m2 = mu2 + rho * (x - mu1) * sigma2/sigma1; 
        tmp = rnorm(1, m2, s2);
        mat(i, 1) = tmp[0];
    }
    return mat;
}')

```

***step2***

write function "GibbsR" to generate a bivariate normal chain whose parameters are the same as GibbsC's.

```{r}
GibbsR <- function(N, mu_i1, mu_i2){
    X <- matrix(0, N, 2)
    rho <- .9
    #correlation
    mu1 <- mu2 <- 0
    sigma1 <-sigma2 <- 1 
    s1 <- sqrt(1-rho^2)*sigma1
    s2 <- sqrt(1-rho^2)*sigma2
    X[1, ] <- c(mu_i1, mu_i2) #initialize
    for (i in 2:N) { 
        y <- X[i-1, 2] 
        m1 <- mu1 + rho * (y - mu2) * sigma1/sigma2 
        X[i, 1] <- rnorm(1, m1, s1) 
        x <- X[i, 1] 
        m2 <- mu2 + rho * (x - mu1) * sigma2/sigma1 
        X[i, 2] <- rnorm(1, m2, s2)
        }
    return(X)
}
```

***step3***

call both functions to generate bivariate normal chains Cnorm and Rnorm with zero means, unit standard deviations, and correlation 0.9 of the same length and initialize value.Draw qqplots of both chains to assess multivariate normality.

```{r,warning=F}
library(Rcpp)
library(microbenchmark)
library(RVAideMemoire)
N <- 5000
burn <- 500
Cnorm <- matrix(0, N, 2)
Rnorm <- matrix(0, N, 2)
Cnorm_b <- matrix(0, N-burn, 2)
Rnorm_b <- matrix(0, N-burn, 2)
Cnorm <- GibbsC(N, 0, 0)
Rnorm <- GibbsR(N, 0, 0)
Cnorm_b <- Cnorm[burn:N ,]
Rnorm_b <- Rnorm[burn:N ,]
mqqnorm(Cnorm_b, main = "Multi-normal Q-Q Plot with C")
mqqnorm(Rnorm_b, main = "Multi-normal Q-Q Plot with R")
qqplot(Cnorm[, 1], 
       Rnorm[, 1], 
       xlab = "Xt with cpp", 
       ylab = "Xt with R")
qqplot(Cnorm[, 2], 
       Rnorm[, 2], 
       xlab = "Yt with cpp", 
       ylab = "Yt with R")
```

**conclusion:** From the qqplots of chains in contrast with bivariate normal distribution we could see that both chains are approximately  follow bivariate normal distribution.The qqplots of Xt and Yt of both chains imply that result of these two method are pretty much the same. 

***step4***

Use function "microbenchmark" to computation time of the two functions

```{r}
ts <- microbenchmark(GibbsC(N, 0, 0), GibbsR(N, 0, 0))
ts
rm(list=ls())
```

**conclusion:** From the outcome of consuming time of both functions we could see that the speed of Cpp way is much faster than that of R way in generating bivariate normal chains
